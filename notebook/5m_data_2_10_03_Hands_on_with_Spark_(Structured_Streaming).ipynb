{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAlbfpGF5aUh"
      },
      "source": [
        "# Hands-on with Spark (Structured Streaming)\n",
        "\n",
        "![spark](https://cdn-images-1.medium.com/max/300/1*c8CtvqKJDVUnMoPGujF5fA.png)\n",
        "\n",
        "In the previous lesson, we learnt about Spark SQL, Dataframes and Pandas API. In this lesson, we will continue with the Structured Streaming.\n",
        "\n",
        "Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. You can express your streaming computation the same way you would express a batch computation on static data. The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive.\n",
        "\n",
        "You can use the Dataset/DataFrame API to express streaming aggregations, event-time windows, stream-to-batch joins, etc. The computation is executed on the same optimized Spark SQL engine. Finally, the system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs. In short, Structured Streaming provides **fast, scalable, fault-tolerant, end-to-end exactly-once** stream processing without the user having to reason about streaming.\n",
        "\n",
        "Internally, by default, Structured Streaming queries are processed using a micro-batch processing engine, which processes data streams as a series of small batch jobs thereby achieving end-to-end latencies as low as 100 milliseconds and exactly-once fault-tolerance guarantees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0M79JSa6Oqv"
      },
      "source": [
        "## Installing and Initializing Spark\n",
        "\n",
        "First, like previously, we'll need to install Spark and its dependencies:\n",
        "\n",
        "1.   Java 8\n",
        "2.   Apache Spark with Hadoop\n",
        "3.   Findspark (used to locate the Spark in the system)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KE17krsq6Rd6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 pyspark-shell'\n",
        "\n",
        "# set the options to connect to our Kafka cluster\n",
        "options = {\n",
        "    # \"kafka.sasl.jaas.config\": 'org.apache.kafka.common.security.scram.ScramLoginModule required username=\"YnJhdmUtZmlzaC0xMTQ2MyQSvwXBuLOQsV1W7YffuC8cDaZcA3fKQwakMhnQGgg\" password=\"MDUxNjc4YzEtYzYxNy00NTE1LWEwNWYtMDBhODRlZmE0OGJm\";',\n",
        "    # \"kafka.sasl.mechanism\": \"SCRAM-SHA-256\",\n",
        "    # \"kafka.security.protocol\" : \"SASL_SSL\",\n",
        "    \"kafka.bootstrap.servers\": 'localhost:9092',\n",
        "    \"subscribe\": 'pizza-orders',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ra8Ncwms6UsD"
      },
      "outputs": [],
      "source": [
        "# import findspark\n",
        "# findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhi1xuPB6VfD"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"LearnSparkStreaming\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "VYTXdMSv6cpC",
        "outputId": "e0817f69-3087-4e2c-e643-5ba3ed477a11"
      },
      "outputs": [],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36cRSgfTNXXA"
      },
      "source": [
        "## Read and Analyze Kafka stream in \"Batch\" Mode\n",
        "\n",
        "Let's start with reading and analyzing our `pizza-orders` kafka topic in the usual \"batch\" mode of Spark SQL and Dataframes. This is akin to the batch queries we did in the previous lesson. In this case we are taking the messages with the earliest to latest offsets of the topic as a single \"batch\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fZDMWC26dJ0"
      },
      "outputs": [],
      "source": [
        "pizza_df = spark.read.format('kafka')\\\n",
        "    .options(**options)\\\n",
        "    .load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJLqrFeXOXpG",
        "outputId": "56b04137-b3cc-4231-e23a-ea7f16977956"
      },
      "outputs": [],
      "source": [
        "pizza_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pizza_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5--uyDySFSEB",
        "outputId": "30d90a0a-8d05-4ae7-8ad6-dcdba0677464"
      },
      "outputs": [],
      "source": [
        "pizza_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_zjPQ6_FUVE"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import from_json, col\n",
        "from pyspark.sql.types import StringType, IntegerType, LongType, DoubleType, StructType, ArrayType, StructField"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owyJGHliQdBG"
      },
      "outputs": [],
      "source": [
        "pizza_schema = StructType([\n",
        "  StructField(\"pizzaName\", StringType()),\n",
        "  StructField(\"additionalToppings\", ArrayType(StringType())),\n",
        "])\n",
        "\n",
        "order_schema = StructType([\n",
        "  StructField(\"address\", StringType()),\n",
        "  StructField(\"id\", IntegerType()),\n",
        "  StructField(\"name\", StringType()),\n",
        "  StructField(\"phoneNumber\", StringType()),\n",
        "  StructField(\"shop\", StringType()),\n",
        "  StructField(\"cost\", DoubleType()),\n",
        "  StructField(\"pizzas\", ArrayType(pizza_schema)),\n",
        "  StructField(\"timestamp\", LongType()),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_KAqsS7Q4_5"
      },
      "outputs": [],
      "source": [
        "parsed_df = pizza_df.select(\"timestamp\", from_json(col(\"value\").cast(\"string\"), order_schema).alias(\"value\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7MWOtWPRM-0",
        "outputId": "af678ae7-e0be-4117-88b9-27cdd9fd8e3d"
      },
      "outputs": [],
      "source": [
        "parsed_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mio4VUyERh1H",
        "outputId": "36c7f7d8-3f03-41dd-9425-5859d45bd940"
      },
      "outputs": [],
      "source": [
        "parsed_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdYUKmrygE7_"
      },
      "source": [
        "We can use _dot notation_ to select the field within a `Struct`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt_TUX3_edrK",
        "outputId": "8907d09f-371c-4c04-8912-a91348b40c3f"
      },
      "outputs": [],
      "source": [
        "parsed_df.select(\"value.cost\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgfNZXpVgXz_"
      },
      "source": [
        "Computing the \"total revenue\" per shop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqZHVVFggUYv",
        "outputId": "5c140144-c405-44d5-ff32-e38bdd0cab0b"
      },
      "outputs": [],
      "source": [
        "parsed_df.groupBy(\"value.shop\").sum(\"value.cost\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GuvYqRChU_v"
      },
      "source": [
        "> 1. Count the no. of orders by shop.\n",
        "> 2. Compute the avg revenue by shop and sort by highest to lowest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEuCpW_Pgk1Q"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import min, max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8IhcetFiY24",
        "outputId": "7af16123-be01-4746-e028-2d0f66212964"
      },
      "outputs": [],
      "source": [
        "parsed_df.select(min(\"timestamp\"), max(\"timestamp\")).show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MDIcbPs74B3"
      },
      "source": [
        "## Read and Analyze Kafka stream in \"Streaming\" Mode\n",
        "\n",
        "The key idea in Structured Streaming is to treat a live data stream as a table that is being continuously appended. This leads to a new stream processing model that is very similar to a batch processing model. You will express your streaming computation as standard batch-like query as on a static table, and Spark runs it as an incremental query on the *unbounded input table*. Let’s understand this model in more detail.\n",
        "\n",
        "Consider the input data stream as the “Input Table”. Every data item that is arriving on the stream is like a new row being appended to the Input Table.\n",
        "\n",
        "![concept](https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png)\n",
        "\n",
        "A query on the input will generate the “Result Table”. Every trigger interval (say, every 1 second), new rows get appended to the Input Table, which eventually updates the Result Table. Whenever the result table gets updated, we would want to write the changed result rows to an external sink.\n",
        "\n",
        "![result table](https://spark.apache.org/docs/latest/img/structured-streaming-model.png)\n",
        "\n",
        "To illustrate the use of this model, let’s understand the model in context of a word count model. The first lines DataFrame is the input table, and the final wordCounts DataFrame is the result table. Note that the query on streaming lines DataFrame to generate wordCounts is exactly the same as it would be a static DataFrame. However, when this query is started, Spark will continuously check for new data from the socket connection. If there is new data, Spark will run an “incremental” query that combines the previous running counts with the new data to compute updated counts, as shown below.\n",
        "\n",
        "![example](https://spark.apache.org/docs/latest/img/structured-streaming-example-model.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9A-q7BY9F_F"
      },
      "source": [
        "Event-time is the time embedded in the data itself. For many applications, you may want to operate on this event-time.\n",
        "\n",
        "For example, if you want to get the number of events generated by IoT devices every minute, then you probably want to use the time when the data was generated (that is, event-time in the data), rather than the time Spark receives them.\n",
        "\n",
        "This event-time is very naturally expressed in this model – each event from the devices is a row in the table, and event-time is a column value in the row. This allows window-based aggregations (e.g. number of events every minute) to be just a special type of grouping and aggregation on the event-time column – each time window is a group and each row can belong to multiple windows/groups. Therefore, such event-time-window-based aggregation queries can be defined consistently on both a static dataset (e.g. from collected device events logs) as well as on a data stream, making the life of the user much easier.\n",
        "\n",
        "Furthermore, this model naturally handles data that has arrived later than expected based on its event-time. Since Spark is updating the Result Table, it has full control over updating old aggregates when there is late data, as well as cleaning up old aggregates to limit the size of intermediate state data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKfF1Mc9STFX"
      },
      "outputs": [],
      "source": [
        "pizza_df = spark.readStream.format('kafka')\\\n",
        "    .options(**options)\\\n",
        "    .load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ss8wgrNDkwuW",
        "outputId": "d0f3d31d-d229-46f7-db9b-9a893ca8f44b"
      },
      "outputs": [],
      "source": [
        "pizza_df.isStreaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grlYCt8Ujg3G",
        "outputId": "3048feba-75ec-40bd-9f1f-6b9bd8788622"
      },
      "outputs": [],
      "source": [
        "pizza_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_vL4jkTjl2v"
      },
      "outputs": [],
      "source": [
        "parsed_df = pizza_df.select(\"timestamp\", from_json(col(\"value\").cast(\"string\"), order_schema).alias(\"value\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox00HZHql7_N",
        "outputId": "b02edc33-ebfd-4867-9141-9036946e2320"
      },
      "outputs": [],
      "source": [
        "parsed_df.writeStream.format(\"console\").start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1ETDCxskFZ9",
        "outputId": "9b7b3a27-13c7-447a-d5cd-2a02d0d55892"
      },
      "outputs": [],
      "source": [
        "parsed_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0nnh-Nrjmxd",
        "outputId": "fba91660-248f-4c34-a615-c86c56fe7edf"
      },
      "outputs": [],
      "source": [
        "query = parsed_df.where(\"value.cost > 10\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "zY8eDSs9kLan",
        "outputId": "c4ae7110-9517-4f0d-bf8a-d767fb4c8c55"
      },
      "outputs": [],
      "source": [
        "query = parsed_df \\\n",
        "    .writeStream \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination() # stops the script from exiting\n",
        "# query.isActive\n",
        "# query.recentProgress\n",
        "# query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query.recentProgress"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### In the next few cells, we will provide a complete example of counting the number of pizza shops with orders, as the orders stream in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First check if any streams are active\n",
        "spark.streams.active"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If there are any active streams, stop them\n",
        "for q in spark.streams.active:\n",
        "    print(f\"Stopping query: {q.name}\")\n",
        "    q.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete example with counting\n",
        "pizza_df = spark.readStream.format('kafka')\\\n",
        "    .options(**options)\\\n",
        "    .load()\n",
        "\n",
        "parsed_df = pizza_df.select(\"timestamp\", from_json(col(\"value\").cast(\"string\"), order_schema).alias(\"value\")) #.groupBy(\"value.shop\").count()\n",
        "\n",
        "shop_counts = parsed_df.groupBy(\"value.shop\").count()\n",
        "\n",
        "# outputMode(\"complete\") rewrites all aggregated results every trigger — good for full snapshots.\n",
        "# outputMode(\"update\") - only updated rows are written\n",
        "query = shop_counts \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query.recentProgress"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You have come to the end of this exercise.\n",
        "\n",
        "To delete the Kafka topic `pizza-orders`, use the command below in your terminal:\n",
        "\n",
        "`./kafka-topics.sh --delete --topic pizza-orders --bootstrap-server localhost:9092`\n",
        "\n",
        "To exit Kafka in your terminal, type `exit`."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "kafka",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
